# Efficient-Multimodal-LLMs-Survey

> **[Efficient Multimodal Large Language Models: A Survey](http://arxiv.org/abs/2312)**[ [arXiv]](http://arxiv.org/abs/2312) 

> *Yizhang Jin<sup>12</sup>, Jian Li<sup>1</sup>, Kai Wu<sup>1</sup>, Yexin Liu<sup>3</sup>, Tianjun Gu<sup>4</sup>,  Muyang He<sup>3</sup>, Bo Zhao<sup>3</sup>, Xin Tan<sup>4</sup>, Zhenye Gan<sup>1</sup>, Yabiao Wang<sup>1</sup>, Chengjie Wang<sup>1</sup>*

> *<sup>1</sup>Tencent YouTu Lab, <sup>2</sup>Shanghai Jiao Tong University, <sup>3</sup>Beijing Academy of Artificial Intelligence, <sup>4</sup>East China Normal University*

<p align="center">
    <img src="./imgs/timeline.png" width="100%" height="100%">
</p>

## ðŸ“Œ What is This Survey About?

In the past year, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in tasks such as visual question answering, visual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of efficient MLLMs. Specifically, we summarize the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications. Finally, we discuss the limitations of current efficient MLLM research and promising future directions.

<p align="center">
    <img src="./imgs/arch.png" width="100%" height="100%">
</p>

**âš¡We will actively maintain this repository and incorporate new research as it emerges**. 